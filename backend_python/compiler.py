# compiler.py
import re
import ast

def clean_name(name):
    # å°†èŠ‚ç‚¹åç§°è½¬åŒ–ä¸ºåˆæ³•çš„ Python å˜é‡å
    return re.sub(r'\W|^(?=\d)', '_', name)

# ==========================================
# ã€æ ¸å¿ƒå‡çº§ã€‘ï¼šæ™ºèƒ½å‚æ•°ç±»å‹æ ¼å¼åŒ–å™¨
# ==========================================
def format_param(val_str, p_type):
    val_str = str(val_str).strip()
    
    # 1. å¿½ç•¥ç©ºå€¼å’Œ None
    if val_str == "" or val_str == "None":
        return None
    
    # 2. å¤„ç†æ˜ç¡®çš„ Bool ç±»å‹
    if p_type == "bool":
        return "True" if val_str.lower() == "true" else "False"
        
    # 3. å¤„ç†æ˜ç¡®çš„æ•´æ•° (è§£å†³ SpinBox ä¼ è¿‡æ¥ 1.0 çš„é—®é¢˜)
    elif p_type == "int":
        try:
            return str(int(float(val_str)))
        except:
            pass
            
    # 4. å¤„ç†æ˜ç¡®çš„æµ®ç‚¹æ•°
    elif p_type == "float":
        try:
            return str(float(val_str))
        except:
            pass

    # 5. ã€æ™ºèƒ½å—…æ¢ã€‘ï¼šå°è¯•å°†å…¶ä½œä¸º Python ä»£ç æ±‚å€¼
    try:
        # å¦‚æœæ˜¯ "5"ï¼Œæ±‚å€¼åæ˜¯ int ç±»å‹çš„ 5
        # å¦‚æœæ˜¯ "(3, 3)"ï¼Œæ±‚å€¼åæ˜¯ tuple ç±»å‹çš„ (3, 3)
        evaluated = ast.literal_eval(val_str)
        
        # å¦‚æœæ±‚å€¼åå‘ç°ç¡®å®æ˜¯ä¸ªçº¯å­—ç¬¦ä¸²ï¼ŒåŠ ä¸Šå•å¼•å·
        if isinstance(evaluated, str):
            return f"'{evaluated}'"
            
        # å¦åˆ™ç›´æ¥è¿”å›å®ƒçš„ä»£ç å½¢æ€
        return str(evaluated)
    except Exception:
        # 6. å¦‚æœæ±‚å€¼æŠ¥é”™ (æ¯”å¦‚ "zeros", "reflect")ï¼Œè¯´æ˜å®ƒæ˜¯æ™®é€šå­—ç¬¦ä¸²ï¼Œå®‰å…¨åŠ å¼•å·
        return f"'{val_str}'"

def generate_pytorch_code(project_data, main_class_name="MyNetwork"):
    code_blocks = []
    code_blocks.append("import torch")
    code_blocks.append("import torch.nn as nn\n")
    
    # ä¼˜å…ˆç¼–è¯‘å­ç©ºé—´ï¼Œç¡®ä¿å®ƒä»¬åœ¨ä¸»ç½‘ç»œä¹‹å‰è¢«å®šä¹‰
    graphs_to_compile = [g for g in project_data.keys() if g != "main"] + ["main"]
    
    for graph_id in graphs_to_compile:
        graph = project_data[graph_id]
        nodes = graph.get("nodes", {})
        conns = graph.get("connections", [])
        
        c_name = main_class_name if graph_id == "main" else f"SubNet_{clean_name(graph_id)}"
        
        init_lines = []
        forward_lines = []
        
        # 1. è§£æ Input
        data_inputs = [nid for nid, info in nodes.items() if info["type"] == "Data Input"]
        def sort_key(nid):
            nid_str = str(nid)
            if nid_str.startswith("input") and nid_str[5:].isdigit():
                return int(nid_str[5:])
            return nodes[nid].get("pos_y", 0)
        data_inputs.sort(key=sort_key)
        
        input_args = ["self"]
        for i, nid in enumerate(data_inputs):
            arg_name = f"x_{i}"
            input_args.append(arg_name)
            forward_lines.append(f"        v_{clean_name(nid)} = {arg_name}")
        
        # 2. æ‹“æ‰‘æ’åº
        in_degree = {nid: 0 for nid in nodes}
        adj = {nid: [] for nid in nodes}
        incoming_ports = {nid: {} for nid in nodes}
        
        for c in conns:
            f, t, tp = c["from"], c["to"], c.get("to_port", 0)
            adj[f].append(t)
            in_degree[t] += 1
            incoming_ports[t][tp] = f
            
        q = [nid for nid in nodes if in_degree[nid] == 0]
        topo_order = []
        while q:
            curr = q.pop(0)
            topo_order.append(curr)
            for neighbor in adj[curr]:
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    q.append(neighbor)
                    
        for nid in nodes:
            if nid not in topo_order: topo_order.append(nid)
                
        # 3. ç”Ÿæˆè¿ç®—ä»£ç 
        for nid in topo_order:
            info = nodes[nid]
            l_type = info["type"]
            if l_type == "Data Input": continue
            
            # æœé›†è¾“å…¥å˜é‡
            req_inputs = []
            ins = info.get("inputs", [])
            req_count = len(ins) if ins else (1 if info.get("main_in") else 0)
            for port in range(req_count):
                src_node = incoming_ports[nid].get(port)
                req_inputs.append(f"v_{clean_name(src_node)}" if src_node else "None")
            
            # ã€åº”ç”¨å‚æ•°æ¸…æ´—é€»è¾‘ã€‘
            p_raw = info.get("params", {})
            params = {}
            for k, v in p_raw.items():
                val_str = v.get("value", "") if isinstance(v, dict) else str(v)
                p_type = v.get("type", "string") if isinstance(v, dict) else "string"
                
                fmt_val = format_param(val_str, p_type)
                if fmt_val is not None:
                    params[k] = fmt_val
            
            out_var = f"v_{clean_name(nid)}"
            
            if l_type == "Data Output":
                ret_val = req_inputs[0] if req_inputs else "None"
                forward_lines.append(f"        return {ret_val}")
                continue
                
            if l_type == "Group":
                sub_class = f"SubNet_{clean_name(nid)}"
                layer_name = f"group_{clean_name(nid)}"
                init_lines.append(f"        self.{layer_name} = {sub_class}()")
                forward_lines.append(f"        {out_var} = self.{layer_name}({', '.join(req_inputs)})")
                
            elif l_type == "Loop":
                sub_class = f"SubNet_{clean_name(nid)}"
                layer_name = f"loop_{clean_name(nid)}"
                iters = params.get("iterations", "3")
                init_lines.append(f"        self.{layer_name} = {sub_class}()")
                forward_lines.append(f"        {out_var} = {req_inputs[0]}")
                forward_lines.append(f"        for _ in range({iters}):")
                forward_lines.append(f"            {out_var} = self.{layer_name}({out_var})")
                
            elif l_type == "Concat":
                dim = params.get("dim", "1")
                forward_lines.append(f"        {out_var} = torch.cat(({', '.join(req_inputs)}), dim={dim})")
                
            elif l_type == "Math":
                op = params.get("op", "'add'").replace("'", "").replace('"', "")
                a, b = req_inputs[0], req_inputs[1] if len(req_inputs) > 1 else "None"
                if "add" in op: forward_lines.append(f"        {out_var} = {a} + {b}")
                elif "sub" in op: forward_lines.append(f"        {out_var} = {a} - {b}")
                elif "mul" in op: forward_lines.append(f"        {out_var} = {a} * {b}")
                elif "div" in op: forward_lines.append(f"        {out_var} = {a} / {b}")
                elif "matmul" in op: forward_lines.append(f"        {out_var} = torch.matmul({a}, {b})")
                
            elif l_type == "Value Display":
                forward_lines.append(f"        {out_var} = {req_inputs[0]}")
                
            else: # å¸¸è§„ PyTorch å±‚
                layer_name = f"op_{clean_name(nid)}"
                clean_args = [f"{k}={v}" for k, v in params.items()]
                init_lines.append(f"        self.{layer_name} = nn.{l_type}({', '.join(clean_args)})")
                forward_lines.append(f"        {out_var} = self.{layer_name}({req_inputs[0]})")
                
        # 4. ç»„è£… Python ç±»
        code_blocks.append(f"class {c_name}(nn.Module):")
        code_blocks.append(f"    def __init__(self):")
        code_blocks.append(f"        super({c_name}, self).__init__()")
        if not init_lines: code_blocks.append(f"        pass")
        else: code_blocks.extend(init_lines)
        code_blocks.append("")
        
        code_blocks.append(f"    def forward({', '.join(input_args)}):")
        if not forward_lines: code_blocks.append(f"        pass")
        else: code_blocks.extend(forward_lines)
        code_blocks.append("\n")
        
    return "\n".join(code_blocks)


# å°†è¿™æ®µä»£ç è¿½åŠ åˆ° compiler.py æ–‡ä»¶çš„æœ€ä¸‹æ–¹

# ==========================================
# è®­ç»ƒä¸“ç”¨ PyTorch ä»£ç ç¼–è¯‘å™¨
# ==========================================
# ==========================================
# è®­ç»ƒä¸“ç”¨ PyTorch ä»£ç ç¼–è¯‘å™¨
# ==========================================
def generate_train_code(project_data, main_class_name="MyNetwork"):
    train_graph = project_data.get("main", {})
    nodes = train_graph.get("nodes", {})

    # 1. æ‰«æè®­ç»ƒç”»å¸ƒï¼Œå¯»æ‰¾å…³é”®ç»„ä»¶
    model_node_name = None
    loss_node = None
    optim_node = None
    dataset_node = None
    target_node = None
    config_node = None

    for nid, info in nodes.items():
        l_type = info.get("type", "")
        if l_type == "Group": model_node_name = info.get("name", nid) 
        elif "Loss" in l_type: loss_node = info
        elif l_type in ["Adadelta", "Adagrad", "Adam", "AdamW", "SGD", "RMSprop"]: optim_node = info
        elif l_type == "Dataset Loader": dataset_node = info
        elif l_type == "Target Loader": target_node = info
        elif l_type == "Training Config": config_node = info

    if not model_node_name:
        raise ValueError("è®­ç»ƒç”»å¸ƒä¸Šæœªæ£€æµ‹åˆ°å¯¼å…¥çš„ç½‘ç»œæ¨¡å‹ï¼è¯·å…ˆå¯¼å…¥ .bpnn æ¨¡å‹å¹¶è¿›è¡Œè¿çº¿ã€‚")

    # 2. æå–å¹¶ç”Ÿæˆç½‘ç»œç»“æ„æ¨¡å‹ä»£ç 
    model_graphs = {}
    if model_node_name in project_data:
        model_graphs["main"] = project_data[model_node_name]
        for k in project_data.keys():
            if k.startswith(model_node_name + "_"):
                model_graphs[k] = project_data[k]
    else:
        model_graphs["main"] = {"nodes": {}, "connections": []}
        
    model_code = generate_pytorch_code(model_graphs, main_class_name)

    # 3. æå–è®­ç»ƒè¶…å‚æ•°å’Œä»£ç å—
    epochs = config_node["params"]["epochs"]["value"] if config_node else "100"
    batch_size = config_node["params"]["batch_size"]["value"] if config_node else "32"
    save_freq = config_node["params"]["save_freq"]["value"] if config_node else "10"
    save_path = config_node["params"]["save_path"]["value"] if config_node else "./weights.pth"
    
    dataset_path = dataset_node["params"]["dataset_path"]["value"] if dataset_node else "./data"
    
    # è·å– Data å’Œ Target çš„è‡ªå®šä¹‰ä»£ç å—
    data_code = dataset_node["params"]["custom_code"]["value"] if dataset_node else "def get_dataloader(path, batch_size):\n    pass"
    target_code = target_node["params"]["custom_code"]["value"] if target_node else "def process_target(targets):\n    return targets"

    # å¤„ç† Loss ä¸ Optimizer å‚æ•°
    loss_type = loss_node["type"] if loss_node else "CrossEntropyLoss"
    loss_args = []
    if loss_node:
        for k, v in loss_node.get("params", {}).items():
            val = v.get("value", "")
            if val != "" and val != "None":
                from compiler import format_param
                fmt = format_param(val, v.get("type", "string"))
                if fmt: loss_args.append(f"{k}={fmt}")
    
    optim_type = optim_node["type"] if optim_node else "Adam"
    optim_args = []
    if optim_node:
        for k, v in optim_node.get("params", {}).items():
            val = v.get("value", "")
            if val != "" and val != "None":
                from compiler import format_param
                fmt = format_param(val, v.get("type", "string"))
                if fmt: optim_args.append(f"{k}={fmt}")

    # 4. ç»„è£…ç»ˆæå¯è¿è¡Œçš„ train.py ä»£ç 
    train_script = f"""{model_code}
import torch.optim as optim

# ==========================================
# 1. æ•°æ®é›†åŠ è½½ä¸é¢„å¤„ç†æ¨¡å—
# ==========================================
{data_code}

{target_code}

# ==========================================
# 2. è®­ç»ƒä¸»å¾ªç¯
# ==========================================
def train():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"æ­£åœ¨ä½¿ç”¨ {{device}} å‡†å¤‡è®­ç»ƒ...")
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = {main_class_name}().to(device)
    
    # å‡†å¤‡æ•°æ®ã€ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    dataloader = get_dataloader(r'{dataset_path}', {batch_size})
    optimizer = optim.{optim_type}(model.parameters(), {', '.join(optim_args)})
    criterion = nn.{loss_type}({', '.join(loss_args)})

    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    for epoch in range({epochs}):
        model.train()
        total_loss = 0.0
        
        for batch_idx, (inputs, targets) in enumerate(dataloader):
            # å°†æ•°æ®å’Œæ ‡ç­¾è½¬ç§»åˆ°è®¾å¤‡
            inputs = inputs.to(device)
            targets = targets.to(device)
            
            # å¯¹æ ‡ç­¾è¿›è¡Œè¿›ä¸€æ­¥å¤„ç† (æ ¹æ® Target Loader çš„é€»è¾‘)
            targets = process_target(targets)

            # å‰å‘ä¼ æ’­ä¸è¯¯å·®è®¡ç®—
            optimizer.zero_grad()
            outputs = model(inputs) # Dataset æµå…¥æ¨¡å‹
            loss = criterion(outputs, targets) # æ¨¡å‹é¢„æµ‹å€¼ä¸ Target çš„è¯¯å·®å¯¹æ¯”
            
            # åå‘ä¼ æ’­ä¸ä¼˜åŒ–
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        # æ‰“å°æ—¥å¿—
        avg_loss = total_loss / len(dataloader)
        print(f"Epoch [{{epoch+1}}/{epochs}] Loss: {{avg_loss:.6f}}")

        # å®šæœŸä¿å­˜æƒé‡
        if (epoch + 1) % {save_freq} == 0:
            import torch
            torch.save(model.state_dict(), r'{save_path}')
            print(f"âœ… é˜¶æ®µæƒé‡å·²ä¿å­˜è‡³ {save_path}")

if __name__ == '__main__':
    train()
"""
    return train_script

# ==========================================
# æ¨ç†éƒ¨ç½²ä¸“ç”¨ PyTorch ä»£ç ç¼–è¯‘å™¨
# ==========================================
def generate_test_code(project_data, main_class_name="MyNetwork"):
    test_graph = project_data.get("main", {})
    nodes = test_graph.get("nodes", {})

    model_node_name = None
    config_node = None

    for nid, info in nodes.items():
        l_type = info.get("type", "")
        if l_type == "Group": model_node_name = info.get("name", nid) 
        elif l_type == "Inference Config": config_node = info

    if not model_node_name:
        raise ValueError("éƒ¨ç½²ç”»å¸ƒä¸Šæœªæ£€æµ‹åˆ°å¯¼å…¥çš„ç½‘ç»œæ¨¡å‹ï¼è¯·å…ˆå¯¼å…¥ .bpnn æ¨¡å‹å¹¶è¿›è¡Œè¿çº¿ã€‚")

    # æå–ç½‘ç»œç»“æ„
    model_graphs = {}
    if model_node_name in project_data:
        model_graphs["main"] = project_data[model_node_name]
        for k in project_data.keys():
            if k.startswith(model_node_name + "_"):
                model_graphs[k] = project_data[k]
    else:
        model_graphs["main"] = {"nodes": {}, "connections": []}
        
    model_code = generate_pytorch_code(model_graphs, main_class_name)

    # æå–æ¨ç†é…ç½®
    weights_path = config_node["params"]["weights_path"]["value"] if config_node else "./weights/model.pth"
    device_str = config_node["params"]["device"]["value"] if config_node else "cuda"
    
    # åŒ…è£…ç±»å (å¦‚ ResNet -> ResNetInference)
    inference_class_name = f"{main_class_name}Inference"

    # ç»„è£…æç®€æ¨ç† API
    test_script = f"""{model_code}

# ==========================================
# æ¨ç†éƒ¨ç½² API ç±»
# ==========================================
class {inference_class_name}:
    def __init__(self, weights_path=r'{weights_path}', device='{device_str}'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        print(f"åˆå§‹åŒ–æ¨ç†å¼•æ“ï¼Œè¿è¡Œè®¾å¤‡: {{self.device}}")

        # åˆå§‹åŒ–ç½‘ç»œç»“æ„
        self.model = {main_class_name}()

        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        try:
            self.model.load_state_dict(torch.load(weights_path, map_location=self.device))
            print("âœ… é¢„è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            print(f"âš ï¸ æƒé‡åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡ã€‚é”™è¯¯ä¿¡æ¯: {{e}}")

        self.model.to(self.device)
        self.model.eval() # å¼€å¯æ¨ç†æ¨¡å¼ï¼Œå†»ç»“ Dropout å’Œ BatchNorm

    @torch.no_grad()
    def generate(self, input_data):
        \"\"\"
        æ‰§è¡Œç¥ç»ç½‘ç»œæ¨ç†
        :param input_data: è¾“å…¥çš„ Tensor æ•°æ®
        :return: ç½‘ç»œçš„é¢„æµ‹è¾“å‡º
        \"\"\"
        if isinstance(input_data, torch.Tensor):
            input_data = input_data.to(self.device)

        # å‰å‘ä¼ æ’­æ¨ç†
        output = self.model(input_data)
        return output

if __name__ == '__main__':
    # å¿«é€Ÿæµ‹è¯•ä»£ç 
    print("--- éƒ¨ç½²æ¨¡å—è¿é€šæ€§æµ‹è¯• ---")
    api = {inference_class_name}()
    
    # æ„å»ºä¼ªè¾“å…¥ (å°ºå¯¸ç”±ä½ åœ¨è“å›¾ä¸­å®šä¹‰)
    dummy_input = torch.randn(1, 3, 224, 224).to(api.device)
    result = api.generate(dummy_input)
    print("é¢„æµ‹è¾“å‡ºå°ºå¯¸:", result.shape)
"""
    return test_script